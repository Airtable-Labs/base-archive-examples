{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for Table: Movies\n",
      "Formatting Linked Records for genres_linked field\n",
      "Formatting Linked Records for collection field\n",
      "Checking s3 for existing CSV\n",
      "Existing file found, uploading new version\n",
      "Getting data for Table: Genre\n",
      "Formatting Linked Records for Movies field\n",
      "Checking s3 for existing CSV\n",
      "Existing file found, uploading new version\n",
      "Getting data for Table: Collections\n",
      "Formatting Linked Records for belongs_to_collection field\n",
      "Checking s3 for existing CSV\n",
      "Existing file found, uploading new version\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv # To Access Environment Variables\n",
    "\n",
    "from s3_helpers import upload_file, add_record_ids, format_linked_records\n",
    "\n",
    "from pyairtable import Base, metadata # To Access Airtable\n",
    "import boto3 # To work with AWS\n",
    "import pandas as pd # To work with Data from Airtable\n",
    "\n",
    "# Declare Variables from .env\n",
    "load_dotenv()\n",
    "AIRTABLE_API_KEY = os.environ.get('AIRTABLE_API_KEY')\n",
    "AIRTABLE_BASE_ID = os.environ.get('AIRTABLE_BASE_ID')\n",
    "S3_BUCKET_NAME = os.environ.get('S3_BUCKET_NAME')\n",
    "DIRECTORIES = os.environ.get('DIRECTORIES').split(\",\")\n",
    "\n",
    "# Declare Amazon S3 Variables\n",
    "s3 = boto3.client('s3') #\n",
    "bucket_name = S3_BUCKET_NAME\n",
    "\n",
    "# Load Base and Base Schema\n",
    "my_base = Base(AIRTABLE_API_KEY, AIRTABLE_BASE_ID)\n",
    "schema = metadata.get_base_schema(my_base)\n",
    "tables = schema['tables']\n",
    "table_ids = [x['id'] for x in tables]\n",
    "\n",
    "# Check if Each Folder Directory Exists, if not make one\n",
    "for dir in DIRECTORIES:\n",
    "    tablesPath = dir\n",
    "    tablesFolderExists = os.path.isdir(tablesPath)\n",
    "\n",
    "    if tablesFolderExists:\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(dir)\n",
    "\n",
    "# For each table in a base\n",
    "for table in tables:\n",
    "\n",
    "    # Set Variables for Table Data\n",
    "    table_name = table['name']\n",
    "    filename = 'Tables/'+table['id']+'.csv'\n",
    "    s3_filename = f'Tables/{table_name}/'+table['id']+'.csv'      \n",
    "    print(f'Getting data for Table: {table_name}')\n",
    "\n",
    "    # Get Table Data and create CSV from JSON\n",
    "    table_data_raw = my_base.all(table['id'])\n",
    "    table_data = add_record_ids(table_data_raw)\n",
    "    table_df = pd.DataFrame(table_data)\n",
    "\n",
    "    # Get Array Fields\n",
    "    table_index = tables.index(table)\n",
    "    table_fields = tables[table_index]['fields']\n",
    "    linked_fields = [x for x in table_fields if x['type'] in 'multipleRecordLinks']\n",
    "    array_field_types = ['multipleRecordLinks','multipleCollaborators','multipleSelects','multipleAttachments','mutipleLookupValues']\n",
    "    array_fields = [x['name'] for x in table_fields if x['type'] in array_field_types]\n",
    "    non_array_fields = [x['name'] for x in table_fields if x['type'] not in array_field_types and table_fields.index(x) != 0]\n",
    "\n",
    "    # Format Linked Records\n",
    "    table_df = format_linked_records(linked_fields,table_df,tables,my_base)\n",
    "\n",
    "    # Format Table Data\n",
    "    table_df.columns = table_df.columns.str.replace(' ', '_') # replace spaces in headers with \"_\"\n",
    "    table_df.columns = table_df.columns.str.lower() # transform all characters to lowercase\n",
    "    table_df = table_df.set_index('airtable_id', drop=False)\n",
    "\n",
    "    # Create CSV\n",
    "    table_csv = table_df.to_csv(filename, index=False)\n",
    "\n",
    "     # Check s3 bucket for contents and instance of CSV\n",
    "    print('Checking s3 for existing CSV')\n",
    "    check_s3 = s3.list_objects_v2(\n",
    "        Bucket=bucket_name,\n",
    "        Prefix=\"Table\"\n",
    "        )\n",
    "    no_content = 'Contents' not in check_s3\n",
    "    file_exists = None\n",
    "\n",
    "    if no_content:\n",
    "        file_exists = False\n",
    "\n",
    "    # If there are contents in the bucket check to see if file exists\n",
    "    else:\n",
    "        local_filename = 'Staging/'+table['id']+'.csv'\n",
    "        file_exists = len([object_summary['Key'] for object_summary in check_s3['Contents'] if s3_filename in object_summary['Key']]) > 0\n",
    "\n",
    "    # If File exists, download file, append, and upload to bucket then remove from staging\n",
    "    if file_exists:\n",
    "        print('Existing file found, uploading new version')\n",
    "        s3.download_file(bucket_name, s3_filename, local_filename)\n",
    "        existing_df = pd.read_csv(local_filename)\n",
    "        frames = [existing_df, table_df]\n",
    "        upload_df = pd.concat(frames).drop_duplicates(subset='airtable_id')  # appends new data \n",
    "        upload_df = upload_df.set_index('airtable_id', drop=False)\n",
    "        upload_df.update(table_df) # updates existing columns\n",
    "        upload_df.to_csv(filename, index=False)\n",
    "        upload_file(filename, bucket_name, s3_filename)\n",
    "        os.remove(local_filename)\n",
    "\n",
    "    \n",
    "    # If no file exists with that table id/name in s3 bucket uplpad the CSV Upload File\n",
    "    else:\n",
    "        print('No existing file found, uploading a new file in s3')\n",
    "        upload_file(filename, bucket_name, s3_filename)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
